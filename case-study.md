# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

### Бюджет и метрика
Наша метрика: потребление памяти при работе программы
Наш бюджет: 70MB

### Ваша находка №1
- Профилировали с помощью ruby-prof
- Обнаружена точка роста: необязательный data.parse
- Решение: удалили parse
- Было: Total: 473954.000000
- Стало: Total: 247170.000000

### Ваша находка №2
- Профилировали с помощью ruby-prof
- Обнаружена точка роста: большое количество объектов, создаваемое через map
- Решение: убраны лишние map'ы
- Было: Total: 247170.000000
- Стало: Total: 234184.000000

### Ваша находка №X
- какой отчёт показал главную точку роста
- как вы решили её оптимизировать
- как изменилась метрика
- как изменился отчёт профилировщика

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*
